{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training data files from the local folder. Show the data type and length of the training data. Then, show an instance of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 75000\n",
      "text_train[0]:\n",
      "b'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[0]:\\n{}\".format(text_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br/>\",b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset was collected such that the positive class and the negative class are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500 50000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Bag-of-words to a toy dataset\n",
    "##### Declare an array of 2 phrases from a quote in Shakespeare's \"As you like it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bards_words = [\"The fool doth think he is wise,\", \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import and instantiate the bag-of-words implementaiton in CountVectorizer and fit it to our toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(bards_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the CountVectorizer consists of tokenization and vocabulary building, which can be accessed in the vocabulary_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'but',\n",
       " 'doth',\n",
       " 'fool',\n",
       " 'he',\n",
       " 'himself',\n",
       " 'is',\n",
       " 'knows',\n",
       " 'man',\n",
       " 'the',\n",
       " 'think',\n",
       " 'to',\n",
       " 'wise']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content: \n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content: \\n{}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the bag-of-words representation by calling the transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"Bag of words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the sparse matrix into a \"dense\" NumPy array using the toarray method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine the vocubulary and document-term matrix together using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>be</th>\n",
       "      <th>but</th>\n",
       "      <th>doth</th>\n",
       "      <th>fool</th>\n",
       "      <th>he</th>\n",
       "      <th>himself</th>\n",
       "      <th>is</th>\n",
       "      <th>knows</th>\n",
       "      <th>man</th>\n",
       "      <th>the</th>\n",
       "      <th>think</th>\n",
       "      <th>to</th>\n",
       "      <th>wise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   be  but  doth  fool  he  himself  is  knows  man  the  think  to  wise\n",
       "0   0    0     1     1   1        0   1      0    0    1      1   0     1\n",
       "1   1    1     0     1   0        1   0      1    1    1      0   1     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(bag_of_words.toarray(),columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words for Movie Reviews\n",
    "#### Import and instantiate the CountVectorizer into our Movie Reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x124255 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10359806 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access the vocabulary using the get_feature_name method of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 124255\n",
      "First 20 features:\n",
      "['00', '000', '0000', '0000000000000000000000000000000001', '0000000000001', '000000001', '000000003', '00000001', '000001745', '00001', '0001', '00015', '0002', '0007', '00083', '000ft', '000s', '000th', '001', '002']\n",
      "Features 20010 to 20030:\n",
      "['cheapen', 'cheapened', 'cheapening', 'cheapens', 'cheaper', 'cheapest', 'cheapie', 'cheapies', 'cheapjack', 'cheaply', 'cheapness', 'cheapo', 'cheapozoid', 'cheapquels', 'cheapskate', 'cheapskates', 'cheapy', 'chearator', 'cheat', 'cheata']\n",
      "Every 2000th feature:\n",
      "['00', '_require_', 'aideed', 'announcement', 'asteroid', 'banquière', 'besieged', 'bollwood', 'btvs', 'carboni', 'chcialbym', 'clotheth', 'consecration', 'cringeful', 'deadness', 'devagan', 'doberman', 'duvall', 'endocrine', 'existent', 'fetiches', 'formatted', 'garard', 'godlie', 'gumshoe', 'heathen', 'honoré', 'immatured', 'interested', 'jewelry', 'kerchner', 'köln', 'leydon', 'lulu', 'mardjono', 'meistersinger', 'misspells', 'mumblecore', 'ngah', 'oedpius', 'overwhelmingly', 'penned', 'pleading', 'previlage', 'quashed', 'recreating', 'reverent', 'ruediger', 'sceme', 'settling', 'silveira', 'soderberghian', 'stagestruck', 'subprime', 'tabloids', 'themself', 'tpf', 'tyzack', 'unrestrained', 'videoed', 'weidler', 'worrisomely', 'zombified']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a LogisticRegression classifier and evaluate using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "#### Build a Multinomial Naive Bayes classifier and evaluate using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "scores = cross_val_score(MultinomialNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Bernoulli Naive Bayes classifier and evaluate using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "scores = cross_val_score(BernoulliNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Set to 5 the minimum number of documents that a token needs to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <75000x44532 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10235504 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the first 50 features, the 20010-20030th features and every 700th feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features: \n",
      "['00', '000', '001', '007', '00am', '00pm', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '1001', '100k', '100th', '100x', '101', '101st', '102', '103', '104', '105', '106', '107', '108', '109', '10am', '10pm', '10s', '10th', '10x', '11', '110', '1100', '110th', '111', '112', '1138', '115', '116', '117', '11pm', '11th']\n",
      "Features 20010 to 20030:\n",
      "['inert', 'inertia', 'inescapable', 'inescapably', 'inevitability', 'inevitable', 'inevitably', 'inexcusable', 'inexcusably', 'inexhaustible', 'inexistent', 'inexorable', 'inexorably', 'inexpensive', 'inexperience', 'inexperienced', 'inexplicable', 'inexplicably', 'inexpressive', 'inextricably']\n",
      "Every 700th feature: \n",
      "['00', 'accountability', 'alienate', 'appetite', 'austen', 'battleground', 'bitten', 'bowel', 'burton', 'cat', 'choreographing', 'collide', 'constipation', 'creatively', 'dashes', 'descended', 'dishing', 'dramatist', 'ejaculation', 'epitomize', 'extinguished', 'figment', 'forgot', 'garnished', 'goofy', 'gw', 'hedy', 'hormones', 'imperfect', 'insomniac', 'janitorial', 'keira', 'lansing', 'linfield', 'mackendrick', 'masterworks', 'miao', 'moorehead', 'natassia', 'nude', 'ott', 'particulars', 'phillipines', 'pop', 'profusely', 'raccoons', 'redolent', 'responding', 'ronno', 'satirist', 'seminal', 'shrews', 'smashed', 'spendthrift', 'stocked', 'superman', 'tashman', 'tickets', 'travelling', 'uncomfortable', 'uprising', 'vivant', 'whine', 'x2']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"First 50 features: \\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature: \\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Multinomial Naive Bayes classifier again and evaluate using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(MultinomialNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Show the stop words from the built_in English list in the feature_extraction.text module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Stopwords: \n",
      "['below', 'moreover', 'keep', 'none', 'fifty', 'has', 'without', 'towards', 'yet', 'whoever', 'rather', 'me', 'last', 'each', 'name', 'once', 'made', 'seemed', 'my', 'else', 'same', 'serious', 'several', 'something', 'who', 'also', 'was', 'were', 'themselves', 'though', 'twelve', 'but', 'ie', 'what', 'which', 'from', 'along', 'two', 'formerly', 'whereafter', 'give', 'itself', 'through', 'whereupon', 'afterwards', 'must', 'or', 'sixty', 'off', 'somehow', 'hasnt', 'that', 'less', 'hereby', 'whence', 'get', 'yours', 'under', 'mine', 'this', 'hundred', 'nobody', 'either', 'an', 'had', 'nine', 'ours', 'any', 'whither', 'otherwise', 'across', 'in', 'those', 'about', 'whole', 'enough', 'next', 'seem', 'is', 'namely', 'whether', 'thru', 'it', 'put', 'you', 'ltd', 'never', 'their', 'yourselves', 'de', 'if', 'anyway', 'for', 'someone', 'whenever', 'always', 'least', 'over', 'herein', 'besides', 'become', 'ever', 'mostly', 'eleven', 'please', 'can', 'both', 'do', 'done', 'per', 'ten', 'anywhere', 'more', 'down', 'beforehand', 'up', 'because', 'one', 'at', 'most', 'these', 'further', 'then', 'due', 'thereafter', 'however', 'hereafter', 'while', 'against', 'so', 'thereby', 'have', 'we', 'whose', 'latterly', 'thence', 'thick', 'call', 'thin', 'such', 'became', 'she', 'thereupon', 'to', 'top', 'until', 'many', 'fill', 'interest', 'whereas', 'our', 'the', 'eg', 'un', 'when', 'after', 'toward', 'again', 'anything', 'out', 'sincere', 'meanwhile', 'beyond', 'everyone', 'everything', 'are', 'go', 'as', 'now', 'side', 'with', 're', 'show', 'him', 'twenty', 'not', 'himself', 'bill', 'fifteen', 'already', 'only', 'nor', 'co', 'couldnt', 'during', 'becoming', 'herself', 'mill', 'others', 'take', 'found', 'us', 'throughout', 'alone', 'anyhow', 'myself', 'although', 'noone', 'they', 'amount', 'nevertheless', 'other', 'first', 'detail', 'even', 'four', 'hereupon', 'into', 'of', 'them', 'well', 'eight', 'together', 'inc', 'no', 'amoungst', 'very', 'five', 'your', 'amongst', 'elsewhere', 'latter', 'since', 'his', 'been', 'bottom', 'behind', 'he', 'move', 'own', 'system', 'whom', 'etc', 'and', 'except', 'con', 'i', 'via', 'on', 'may', 'wherein', 'full', 'by', 'neither', 'see', 'nothing', 'than', 'six', 'a', 'beside', 'part', 'sometimes', 'too', 'where', 'another', 'there', 'hence', 'describe', 'perhaps', 'could', 'ourselves', 'therein', 'every', 'former', 'much', 'fire', 'back', 'should', 'thus', 'often', 'nowhere', 'anyone', 'front', 'how', 'within', 'everywhere', 'before', 'wherever', 'among', 'three', 'still', 'be', 'indeed', 'might', 'upon', 'between', 'becomes', 'forty', 'find', 'its', 'somewhere', 'onto', 'almost', 'seems', 'empty', 'third', 'few', 'cry', 'here', 'some', 'yourself', 'being', 'whereby', 'all', 'cant', 'above', 'am', 'seeming', 'her', 'why', 'will', 'would', 'sometime', 'whatever', 'hers', 'therefore', 'around', 'cannot']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Stopwords: \\n{}\".format(list(ENGLISH_STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove English stop words from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<75000x44223 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6621682 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5,stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, build a Multinomial Naive Bayes classifier again and evaluate using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(MultinomialNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency\n",
    "#### Import and instantiate the TfidfVectorizer and setting the min_df=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x44532 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 10235504 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, evaluate the Multinomial Naive Bayes classifier again  using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(MultinomialNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect some of the words with the lowest and highest tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['suplexes' 'acquiesce' 'poffysmoviemania' 'avenged' 'invocus' 'bookcase'\n",
      " 'zantara' 'boozed' 'brutes' 'simpatico' 'pragmatism' 'unrivalled'\n",
      " 'enrages' 'trumpeted' 'flowering' 'fearfully' 'reveries' 'authorial'\n",
      " 'décor' 'clobber']\n",
      "Features with highest tfidf:\n",
      "['grease' 'halloweentown' 'smallville' 'frasier' 'stinks' 'lupin' 'gta'\n",
      " 'tomie' 'lucy' 'skulls' 'scanners' 'doodlebops' 'ling' 'nr' 'yadda' 'ha'\n",
      " 'pokemon' 'freakazoid' 'click' 'wicked']\n"
     ]
    }
   ],
   "source": [
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect some of the words with the lowest idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'br' 'one' 'be' 'have' 'are' 'film' 'you'\n",
      " 'all' 'at' 'an' 'by' 'from' 'so' 'like' 'who' 'there' 'they' 'his' 'if'\n",
      " 'out' 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'can' 'good' 'when'\n",
      " 'more' 'up' 'time' 'very' 'even' 'only' 'no' 'see' 'would' 'my' 'story'\n",
      " 'really' 'which' 'well' 'had' 'me' 'than' 'their' 'much' 'were' 'get'\n",
      " 'other' 'do' 'been' 'most' 'also' 'into' 'don' 'her' 'first' 'great' 'how'\n",
      " 'made' 'people' 'will' 'make' 'because' 'way' 'could' 'bad' 'we' 'after'\n",
      " 'them' 'too' 'any' 'then' 'movies' 'watch' 'she' 'think' 'seen' 'acting'\n",
      " 'its']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words with More Than One Word (n-Grams)\n",
    "##### Applying it again to the toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bards_words;\n",
      "['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n"
     ]
    }
   ],
   "source": [
    "print(\"bards_words;\\n{}\".format(bards_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the unigrams in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the unigrams and bigrams in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27\n",
      "Vocabulary:\n",
      "['be', 'be fool', 'but', 'but the', 'doth', 'doth think', 'fool', 'fool doth', 'he', 'he is', 'himself', 'himself to', 'is', 'is wise', 'knows', 'knows himself', 'man', 'man knows', 'the', 'the fool', 'the wise', 'think', 'think he', 'to', 'to be', 'wise', 'wise man']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the \"dense\" NumPy array using the toarray method of the transformed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data(dense):\n",
      "[[0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0]\n",
      " [1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed data(dense):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the unigrams, bigrams and trigrams in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Vocabulary:\n",
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, try out the TfidfVectorizer on the Movie Reviews dataset and find the best setting of the n-gram range\n",
    "(1) Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x377417 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 22396054 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(MultinomialNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Unigrams, Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x710857 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 28414792 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(MultinomialNB(),X_train,y_train,cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
